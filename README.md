# Diff-DiT: Scalable Diffusion Models with Differential Transformers.

### Differential Attention Meets Diffusion Transformers (DiTs).

The repo currently has pytorch implementation of Diff-DiT's that are drop in replacement for DiT's with various projects.

A PyTorch implementation of **Diffusion Transformers (DiTs)** enhanced with the **Differential Attention Block** from **Differential Transformers (Diff-Transformer)**.

---

## ðŸš§ Upcoming Features

### ðŸ”œ Training & Inference Scripts
### ðŸ”œ Architetural Enhancements
### ðŸ”œ Flash Attention Implementation
### ðŸ”œ JAX Implementation
### ðŸ”œ CUDA Kernel Optimizations

These will be modular and plug-and-play drop in replacement with DiT's.

---

## ðŸ“š References

### ðŸ”— Official Implementations

- **DiT (Diffusion Transformer)**  
  [facebookresearch/DiT](https://github.com/facebookresearch/DiT)

- **Diff-Transformer (Differential Attention)**  
  [microsoft/unilm - Diff-Transformer](https://github.com/microsoft/unilm/tree/master/Diff-Transformer)

- **Vision Transformer (ViT)**  
  [timm - Vision Transformer](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py)

### ðŸ“„ Research Papers

- **DiT**  
  [Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748v2)

- **Diff-Transformer**  
  [Differential Transformers](https://arxiv.org/abs/2410.05258v2)

- **Vision Transformer**  
  [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929v2)

---

## ðŸ”’ License

This source code is licensed under the license found in the [LICENSE](./LICENSE) file in the root directory of this source tree.

---

